{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENV SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/dayanabenny/.venv/lib/python3.9/site-packages (3.6.0)\n",
      "Requirement already satisfied: transformers in /Users/dayanabenny/.venv/lib/python3.9/site-packages (4.53.0)\n",
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp39-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: cadquery in /Users/dayanabenny/.venv/lib/python3.9/site-packages (2.5.2)\n",
      "Requirement already satisfied: matplotlib in /Users/dayanabenny/.venv/lib/python3.9/site-packages (3.9.4)\n",
      "Requirement already satisfied: filelock in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from datasets) (0.33.1)\n",
      "Requirement already satisfied: packaging in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: cadquery-ocp<7.8,>=7.7.0 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from cadquery) (7.7.2)\n",
      "Requirement already satisfied: ezdxf in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from cadquery) (1.4.2)\n",
      "Requirement already satisfied: multimethod<2.0,>=1.11 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from cadquery) (1.12)\n",
      "Requirement already satisfied: nlopt<3.0,>=2.9.0 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from cadquery) (2.9.1)\n",
      "Requirement already satisfied: typish in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from cadquery) (1.9.3)\n",
      "Requirement already satisfied: casadi in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from cadquery) (3.7.0)\n",
      "Requirement already satisfied: path in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from cadquery) (17.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dayanabenny/.venv/lib/python3.9/site-packages (from pandas->datasets) (2025.2)\n",
      "Downloading torch-2.7.1-cp39-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, networkx, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [torch]32m3/4\u001b[0m [torch]kx]\n",
      "\u001b[1A\u001b[2KSuccessfully installed mpmath-1.3.0 networkx-3.2.1 sympy-1.14.0 torch-2.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers torch cadquery matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "#import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded!\n",
      "Train samples: 147289\n",
      "Test samples: 7355\n"
     ]
    }
   ],
   "source": [
    "# Load train and test splits of the dataset\n",
    "\n",
    "train_ds, test_ds = load_dataset(\"CADCODER/GenCAD-Code\", split=[\"train\", \"test\"])\n",
    "\n",
    "print(\"Dataset loaded!\")\n",
    "print(f\"Train samples: {len(train_ds)}\")\n",
    "print(f\"Test samples: {len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corresponding CadQuery code:\n",
      "\n",
      "import cadquery as cq\n",
      "# Generating a workplane for sketch 0\n",
      "wp_sketch0 = cq.Workplane(cq.Plane(cq.Vector(-0.015625, -0.0078125, 0.0), cq.Vector(1.0, 0.0, 0.0), cq.Vector(0.0, 0.0, 1.0)))\n",
      "loop0=wp_sketch0.moveTo(0.0, 0.0).threePointArc((0.0007948582418457166, -0.0019189575476279677), (0.0027138157894736844, -0.0027138157894736844)).lineTo(0.021217105263157895, -0.0027138157894736844).threePointArc((0.022787161438489866, -0.00206347722796355), (0.0234375, -0.000493421052631579)).lineTo(0.0234375, 0.018256578947368422).threePointArc((0.02283825686147997, 0.019949990385858287), (0.021217105263157895, 0.020723684210526318)).lineTo(0.0022203947368421052, 0.020723684210526318).threePointArc((0.0005992431385200307, 0.019949990385858287), (0.0, 0.018256578947368422)).lineTo(0.0, 0.0).close()\n",
      "solid0=wp_sketch0.add(loop0).extrude(0.75)\n",
      "solid=solid0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = train_ds[0]\n",
    "\n",
    "plt.imshow(sample['image'])\n",
    "plt.axis('off')\n",
    "plt.title(\"Sample Image\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCorresponding CadQuery code:\\n\")\n",
    "print(sample['cadquery'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61683c02a8242d5b49cb1932bf2e408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fc5a01afdf47ca942c8cea0349fa47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/982M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70564a93c7145fa95164c48b5c3b913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dayanabenny/.venv/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b22160a7714619951faae518f8edf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/241 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8257f74a87c649d8b2afebb1f2e53d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/982M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2c72612b224ed8a69d7ea34ce065a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de0b12e1c9b48749681bd3564c481b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1273aa25a79b4ec2a0c85162f388ff39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723143014c514a50815edff4374642da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D(nf=2304, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=768)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (crossattention): GPT2Attention(\n",
       "            (c_attn): Conv1D(nf=1536, nx=768)\n",
       "            (q_attn): Conv1D(nf=768, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=768)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D(nf=3072, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=3072)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load pretrained model and processors\n",
    "model_name = \"nlpconnect/vit-gpt2-image-captioning\"  # image captioning model as baseline\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CadQueryDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, feature_extractor, tokenizer, max_target_length=128):\n",
    "        self.dataset = hf_dataset\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_target_length = max_target_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image']\n",
    "        code = item['cadquery']\n",
    "    \n",
    "        # preprocess image\n",
    "        pixel_values = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "    \n",
    "        # tokenize code with padding and attention mask\n",
    "        tokenized = self.tokenizer(code,\n",
    "                                   max_length=self.max_target_length,\n",
    "                                   padding=\"max_length\",\n",
    "                                   truncation=True,\n",
    "                                   return_tensors=\"pt\")\n",
    "    \n",
    "        labels = tokenized.input_ids.squeeze()\n",
    "        attention_mask = tokenized.attention_mask.squeeze()\n",
    "    \n",
    "        # Replace pad token id with -100 to ignore in loss\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "\n",
    "train_dataset = CadQueryDataset(train_ds, feature_extractor, tokenizer)\n",
    "test_dataset = CadQueryDataset(test_ds, feature_extractor, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated CadQuery code sample:\n",
      "\n",
      "a piece of paper with a mouse on it \n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "generated_codes = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    pixel_values = batch['pixel_values'].to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(pixel_values, max_length=128)\n",
    "    preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    generated_codes.extend(preds)\n",
    "    break  # demo only one batch\n",
    "\n",
    "print(\"\\nGenerated CadQuery code sample:\\n\")\n",
    "print(generated_codes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from metrics.valid_syntax_rate import evaluate_syntax_rate_simple\n",
    "from metrics.best_iou import get_iou_best\n",
    "\n",
    "reference_code = test_ds[0]['cadquery']\n",
    "generated_code = generated_codes[0]\n",
    "\n",
    "vsr = evaluate_syntax_rate_simple({\"sample_code\": generated_code})\n",
    "print(f\"Valid Syntax Rate: {vsr}\")\n",
    "\n",
    "try:\n",
    "    iou = get_iou_best(reference_code, generated_code)\n",
    "    print(f\"IOU between reference and generated: {iou}\")\n",
    "except Exception as e:\n",
    "    print(f\"IOU evaluation failed due to invalid generated code: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install uv (or do it you're own way)\n",
    "2. Run `uv sync`\n",
    "3. Run `source .venv/bin/activate`\n",
    "\n",
    "You're good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "The Task : Create the best CadQuery code generator model. \n",
    "\n",
    "1. Load the dataset (147K pairs of Images/CadQuery code).\n",
    "2. Create a baseline model and evaluate it with the given metrics.\n",
    "3. Enhance by any manner the baseline model and evaluate it again.\n",
    "4. Explain you choices and possible bottlenecks. \n",
    "5. Show what enhancements you would have done if you had more time.\n",
    "\n",
    "You can do *WHATEVER* you want, be creative, result is not what matters the most. \n",
    "Creating new model architectures, reusing ones you used in the past, fine-tuning, etc...\n",
    "\n",
    "If you are GPU poor, there are solutions. Absolute value is not what matters, relative value between baseline and enhanced model is what matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oweindourneau/projects/mecagent-technical-test/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"CADCODER/GenCAD-Code\", num_proc=16, split=[\"train\", \"test\"], cache_dir=\"/Volumes/BIG-DATA/HUGGINGFACE_CACHE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Valid Syntax Rate metric assess the validity of the code by executing and checking if error are returned.\n",
    "2. Best IOU assess the similarity between the meshes generated by the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.valid_syntax_rate import evaluate_syntax_rate_simple\n",
    "from metrics.best_iou import get_iou_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Syntax Rate: 1.0\n",
      "IOU: 0.5834943417057687\n"
     ]
    }
   ],
   "source": [
    "## Example usage of the metrics\n",
    "sample_code = \"\"\"\n",
    "height = 60.0\n",
    "width = 80.0\n",
    "thickness = 10.0\n",
    "diameter = 22.0\n",
    "\n",
    "# make the base\n",
    "result = (\n",
    "    cq.Workplane(\"XY\")\n",
    "    .box(height, width, thickness)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sample_code_2 = \"\"\"\n",
    " height = 60.0\n",
    " width = 80.0\n",
    " thickness = 10.0\n",
    " diameter = 22.0\n",
    " padding = 12.0\n",
    "\n",
    " # make the base\n",
    " result = (\n",
    "     cq.Workplane(\"XY\")\n",
    "     .box(height, width, thickness)\n",
    "     .faces(\">Z\")\n",
    "     .workplane()\n",
    "     .hole(diameter)\n",
    "     .faces(\">Z\")\n",
    "     .workplane()\n",
    "     .rect(height - padding, width - padding, forConstruction=True)\n",
    "     .vertices()\n",
    "     .cboreHole(2.4, 4.4, 2.1)\n",
    " )\n",
    "\"\"\"\n",
    "codes = {\n",
    "    \"sample_code\": sample_code,\n",
    "    \"sample_code_2\": sample_code_2,\n",
    "}\n",
    "vsr = evaluate_syntax_rate_simple(codes)\n",
    "print(\"Valid Syntax Rate:\", vsr)\n",
    "iou = get_iou_best(sample_code, sample_code_2)\n",
    "print(\"IOU:\", iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have Fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
